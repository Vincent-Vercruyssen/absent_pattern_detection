""" kNN based anomaly detection.

Reference:
    S. Ramaswamy, R. Rastogi, and K. Shim. Efficient algorithms for mining outliers from large data sets.
    In Proceedings of the 2000 ACM SIGMOD international conference on Management of data, vol. 29, no. 2. ACM, 2000, pp. 427â€“438.

"""

# Authors: Vincent Vercruyssen, 2018.

import math
import numpy as np
from tqdm import tqdm

from sklearn.neighbors import BallTree
from sklearn.metrics.pairwise import cosine_similarity
from .BaseDetector import BaseDetector
from .utils.validation import check_X_y


# -------------
# CLASSES
# -------------

class kNNo(BaseDetector):
    """ k-nearest neighbors outlier detection (kNNo)

    Parameters
    ----------
    k : int (default=10)
        Number of nearest neighbors.

    contamination : float (default=0.1)
        Estimate of the expected percentage of anomalies in the data.

    metric : string (default=euclidean)
        Distance metric for constructing the BallTree: euclidean, cosine

    Comments
    --------
    - The parameter k should be carefully chosen depending on whether you want
    to predict on a separate test set or not.
    - The number of neighbors cannot be larger than the number of instances in
    the data: automatically correct if necessary.
    """

    def __init__(self, k=10, contamination=0.1, metric='euclidean',
                 tol=1e-8, verbose=False):
        super(kNNo, self).__init__()

        self.k = int(k)
        self.contamination = float(contamination)
        if metric not in BallTree.valid_metrics + ['cosine']:
            raise BaseException('Invalid distance metric!')
        self.metric = str(metric)

        self.adapt_nn = False
        self.tol = float(tol)
        self.verbose = bool(verbose)

    def fit_predict(self, X, y=None):
        """ Fit the model to the training set X and returns the anomaly score
            of the instances in X.

        :param X : np.array(), shape (n_samples, n_features)
            The samples to compute anomaly score w.r.t. the training samples.
        :param y : np.array(), shape (n_samples), default = None
            Labels for examples in X.

        :returns y_score : np.array(), shape (n_samples)
            Anomaly score for the examples in X.
        :returns y_pred : np.array(), shape (n_samples)
            Returns -1 for inliers and +1 for anomalies/outliers.
        """

        X, y = check_X_y(X, y)
        self.adapt_nn = True

        return self.fit(X, y).predict(X)

    def fit(self, X, y=None):
        """ Fit the model using data in X.

        :param X : np.array(), shape (n_samples, n_features)
            The samples to compute anomaly score w.r.t. the training samples.
        :param y : np.array(), shape (n_samples), default = None
            Labels for examples in X.

        :returns self : object
        """

        X, y = check_X_y(X, y)
        n, _ = X.shape

        self.nn = self._check_valid_number_of_neighbors(n)
        if self.metric in BallTree.valid_metrics:
            self.tree = BallTree(X, leaf_size=32, metric=self.metric)
        elif self.metric == 'cosine':
            def angular_distance_metric(a, b):
                # in order to be used with BallTree, must be a true distance metric!
                # satisfy: (1) non-negativity, (2) identify, (3) symmetry, (4) triangle inequality
                cs = cosine_similarity(a.reshape(1, -1), b.reshape(1, -1)).flatten()[-1]
                if abs(cs - 1.0) < self.tol:
                    cs = 1.0
                return float(math.acos(cs) / math.pi)
            self.tree = BallTree(X, leaf_size=32, metric='pyfunc', func=angular_distance_metric)

        return self

    def predict(self, X):
        """ Compute the anomaly score + predict the label of instances in X.

        :returns y_score : np.array(), shape (n_samples)
            Anomaly score for the examples in X.
        :returns y_pred : np.array(), shape (n_samples)
            Returns -1 for inliers and +1 for anomalies/outliers.
        """

        X, y = check_X_y(X, None)
        n, _ = X.shape

        # compute the outlier score
        knn_score = np.zeros(n, dtype=float)
        for i, x in tqdm(enumerate(X), disable=not(self.verbose)):
            if self.adapt_nn:
                self.nn += 1
            dist, _ = self.tree.query([x], k=max(2, self.nn))    # TODO: quite slow for larger data sets
            knn_score[i] = dist.flatten()[-1]

        y_score = (knn_score - min(knn_score)) / (max(knn_score) - min(knn_score))

        # prediction threshold + absolute predictions
        self.threshold = np.sort(y_score)[int(n * (1.0 - self.contamination))]
        y_pred = np.ones(n, dtype=float)
        y_pred[y_score < self.threshold] = -1

        return y_score, y_pred

    def _check_valid_number_of_neighbors(self, n_samples):
        """ Check if the number of nearest neighbors is valid and correct.

        :param n_samples : int
            Number of samples in the data.
        """

        return min(n_samples, self.k)
